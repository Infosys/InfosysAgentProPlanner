# Â© 2024-25 Infosys Limited, Bangalore, India. All Rights Reserved.
react_system_prompt_generator = """\
**Objective:** Your goal is to create a high-quality, detailed descriptive system prompt for the AI Agent based on the provided information. \
Follow a logical, step-by-step reasoning process to ensure the output is precise and comprehensive.

---

### Input Parameters:
1. **Agent Name:**
   {agent_name}

2. **Agent Goal:**
   {agent_goal}

3. **Workflow Description:**
   {workflow_description}

4. **Tools:**
   {tool_prompt}

---

### Task Breakdown:

#### Step 1: Understand the Agent's Goal and Workflow
- **Identify the Problem or Challenge:**
  What specific problem or challenge is this agent intended to address?
- **Define the Desired Outcomes:**
  What are the key objectives and expected results of the agent's actions within the workflow?

#### Step 2: Analyze the Workflow
- **Decompose the Workflow:**
  Break down the process into sequential steps.
- **Identify Decision Points:**
  Highlight critical points where the agent needs to make decisions or adapt its actions.
- **Extract Key Guidelines:**
  Determine the essential principles and rules that must guide the agent within the workflow.

#### Step 3: Assess Tool Capabilities
- **Evaluate Tool Functionality:**
  For each tool mentioned in the `tool_prompt`, specify its key features and capabilities.
- **Integration with Workflow:**
  Explain how each tool can be effectively utilized in the workflow to achieve the desired outcomes.
- **Limitations and Constraints:**
  Identify any limitations or constraints in the use of these tools. If a tool is unavailable, note this explicitly without further assumptions.
- If no tools are present so the will not have any capability other than basic chating. If user ask for any thing outside general conversation tell him you are not having the appopriate capabilities.
- **Note** - You are not allowed to use your bulitin capabilities at all.


#### Step 4: Construct the Agent's Description
- **Role and Responsibilities:**
  Clearly define the agent's role in addressing the problem, executing the workflow, and achieving the stated goals.
- **Expertise in Tools:**
  Highlight the agent's ability to leverage the provided tools effectively while adhering to the workflow.
- **Emphasize Guidelines and Outcomes:**
  Stress the importance of following the workflow's rules and achieving the desired outcomes.
- **Clarity and Structure:**
  Ensure the description is logically organized, easy to follow, and unambiguous.

---

### Output Format:

**Agent Name**
{agent_name}

**Goal to Achieve for the Workflow**
- Provide a clear and concise statement of the agent's objectives.

**Guidelines on Tools Provided by the User**
- Summarize the key functionalities and limitations of the tools in the context of the workflow.
- If no tools are provided, the agent should respond that it does not have the capability to answer goal-specific questions.

**Step-by-Step Task Description**
- Detail the workflow steps and how the agent should perform each step using the available tools.

**Additional Relevant Information**
- Include any additional details essential for the agent to perform optimally.

---

**Note:** Follow a step-by-step reasoning process while generating the description. \
Ensure the output is clear, structured, and relevant to the provided inputs. Avoid including extraneous information.

Only return the `SYSTEM PROMPT` for the Agent following the specified Output Format.
"""

tool_prompt_generator = """\
**User Inputs:**
# Tool Description
{tool_description}

# Tool Code
{tool_code_str}

**Task:**
You are a professional code assistant. Please follow the steps outlined below:

---

### Step-by-Step Instructions:

#### Step 1:
Consider the Tool Code provided by the user, which is a string representing a function and and Tool Description.

#### Step 2:
Analyze the function defined in the Tool Code to understand its purpose, parameters, and return values.

#### Step 3:
Generate a well-structured docstring for the function (take into account the provided tool description), \
adhering strictly to the output format. \
The output should **only** include the docstring and exclude any comments, warnings, or other expressions.

#### Step 4:
Follow this specific format for the docstring:
'''
<Detailed description of the function's purpose.>

Args:
    <parameter_name> (<parameter_type>): <description of the parameter>.
    <parameter_name> (<parameter_type>): <description of the parameter>.

Returns:
    <return_type>: <description of what the function returns>.
'''

#### Step 5:
Use the analysis from the Tool Code to generate the required docstring. \
Ensure it follows the specified format and concisely describes the function's purpose, parameters, and return values.\
Ensure that the number of characters in the generated docstring is less than 1010 characters.

---


**Output:**
Provide the well-formatted docstring based on the function described in Tool Code (Only return the docstring).
"""

multi_agent_planner_system_prompt_generator_prompt = """
## Objective
Generate a clear and precise **SYSTEM PROMPT** for the **planner agent** based on the following use case requirements:
- **Clarity and Precision**: Ensure the SYSTEM PROMPT is unambiguous and easy to understand.
- **Incorporate User Inputs**: Include all relevant information from the provided user inputs.
- **Comprehensiveness**: The final output should be thorough and cover all necessary aspects.


**Note**: The planner agent is designed to:
- Develop a simple, step-by-step plan.
- Ensure the plan involves individual tasks that, if executed correctly, will yield the correct answer. Do not add any superfluous steps. Each step should start with "STEP #: [Step Description]".
- Ensure the final step will yield the final answer.
- Make sure that each step contains all the necessary information, do not skip steps. **DO NOT attempt to solve the step, just return the steps.**


## User Inputs
Please consider the following details:

### Use Case Description
{agent_goal}

### Workflow Description
{workflow_description}

### Tools
{tools_prompt}


## Instructions
- **Task**: Create a SYSTEM PROMPT for the planner agent that aligns with the information provided in the User Inputs section.
- **Recommended Template**:
  1. **Agent Name**: Recommend a name for the agent (Must include `Planner` in the name).
  2. **Agent Role**: Introduce the planner agent by name and describe its skills.
  3. **Goal Specification**: Clearly state the planner agent's goal.
  4. **Guidelines**: Provide step-by-step instructions or guidelines on how the agent should develop the plan. Clearly instruct the agent to:
     - Include all necessary details from the past conversation or ongoing conversation in the steps.
     - **DO NOT attempt to solve the step, just return the steps.**
     - **If the user's query can be solved using the tools, return the steps to solve the query using the tools.**
     - **If the user's query is related to the agent's goal, workflow description, tools, or domain, return the steps to solve the query.**
     - **If the user's query is not related to the agent's goal, workflow description, tools, or domain, return an empty list without any steps.**
  5. **Output Format**: This agent is expected to return output in the following format:
    ```json
    {{
        "plan": ["STEP 1: [Step Description]", "STEP 2: [Step Description]", ...]
    }}
    ```
- **Response Format**:
  - Present the SYSTEM PROMPT in a clear and organized manner.
  - Use appropriate headings and bullet points where necessary.
  - The generated only the system prompt in markdown format, **do not wrap it in ```plaintext ``` notation**.
  - **Do not include any example(s), explanations, or notes in the SYSTEM PROMPT.**


**SYSTEM PROMPT:**
"""

multi_agent_executor_system_prompt_generator_prompt = """
## Objective
Generate a clear and precise **SYSTEM PROMPT** for the **executor agent** based on the following use case requirements:
- **Clarity and Precision**: Ensure the SYSTEM PROMPT is unambiguous and easy to understand.
- **Incorporate User Inputs**: Include all relevant information from the provided user inputs.
- **Comprehensiveness**: The final output should be thorough and cover all necessary aspects.

**Note**: The executor agent is designed to:
- Accept an execution plan from the user. The steps in the plan will start with "STEP #: [Step Description]".
- Process the current step in the plan. The agent will be provided the entire plan up to and including the current step, as well as the output from any previous steps.
- Invoke one or more tools to complete the current step. The agent should select the appropriate tool(s) based on the current step's description.
- Return only the result of the current step. DO NOT execute any other step(s) other than the current step.
- Ensure that the executor agent includes the exact response received from the invoked tool(s) in its output without modification or omission. The final response must explicitly contain the tool's response exactly as returned.

## User Inputs
Please consider the following details:

### Use Case Description
{agent_goal}

### Workflow Description
{workflow_description}

### Tools
{tools_prompt}


## Instructions
- **Task**: Create a SYSTEM PROMPT for the executor agent that aligns with the information provided.
- **Recommended Template**:
  1. **Agent Name**: Recommend a name for the agent.
  2. **Agent Role**: Introduce the executor agent role by describing its skills.
  3. **Goal Specification**: Clearly state the executor agent's goal.
- **Response Format**:
  - Present the SYSTEM PROMPT in a clear and organized manner.
  - Use appropriate headings and bullet points where necessary.
  - Do not include any example(s), explanations or notes outside of the SYSTEM PROMPT.


**SYSTEM PROMPT:**

"""

multi_agent_general_llm_system_prompt_generator_prompt = """
## Objective
Generate a precise and unambiguous SYSTEM PROMPT for the General Query Handler based on the provided use case details.

## Key Considerations
- **Clarity and Precision**: Ensure the SYSTEM PROMPT is easy to understand and unambiguous.
- **Incorporate User Inputs**: Include all relevant details from the provided inputs.
  **Note:** General Query Handler does not require any tools; it should only generate appropriate responses to user queries.
  Responses should be polite and, where possible, highlight the objective of the Agentic Application.
- **Comprehensiveness**: Ensure the SYSTEM PROMPT fully captures the General Query Handler's purpose and scope.

**Note:** General Query Handler is designed to:
- Respond to general user queries, which may include greetings, feedback, and appreciation.
- Engage in getting to know each other type of conversations.
- Answer queries related to the agent itself, such as its expertise or purpose.
- Respond to the query that are related to the agent's goal, agent's role, workflow description of the agent
- **NOTE** If the query is not related to the agent's goal, agent's role, workflow description of the agent, and tools it has access to and it requires EXTERNAL KNOWLEDGE, DO NOT give a response to such a type of query; just politely give some appropriate message that you are not capable of responding to such type of query.
- **NOTE** If the input query is outside the scope of general queries, getting to know each other type of conversations, or questions about the agent itself, respond politely that it is not capable of answering such queries or requests.

## User Inputs
Consider the following details when generating the SYSTEM PROMPT:

### Agentic Application Name
{agent_name}

### Agentic Application Goal
{agent_goal}

### Tools
{tools_prompt}

## Instructions
- **Task**: Create a SYSTEM PROMPT for the General Query Handler that aligns with the provided details.
  **General Query Handler does not use any tools; it should only generate appropriate responses to user queries.
  Responses should be polite and, where possible, highlight the objective of the Agentic Application.**
  - Explicitly instruct the General Query Handler to avoid identifying itself as a "General Query Handler" when responding to user queries about the agent's identity or purpose. Instead, it should describe the overall goal, role, and workflow of the multi-agent system as a whole.
  - Ensure responses emphasize the collective objectives and capabilities of the multi-agent system rather than the specific role of the General Query Handler.

- **Recommended Template**:
  1. **General Query Handler's Role**: Describe the General Query Handler's capabilities.
  2. **Goal Specification**: Clearly define the General Query Handler's objective.
  3. **Application Goal**: Describe the `Agentic Application Goal` and the tools that the agent can leverage (Include `Agentic Application Name`)

- **Response Format**:
  - Present the SYSTEM PROMPT in a clear and organized manner.
  - Use appropriate headings and bullet points where necessary.
  - Do not include examples, explanations, or notes outside of the SYSTEM PROMPT.

## SYSTEM PROMPT:

"""

response_generator_agent_system_prompt = '''
## Objective
Generate a clear and precise **SYSTEM PROMPT** for the **response generator agent** based on the following use case requirements:
- **Clarity and Precision**: Ensure the SYSTEM PROMPT is unambiguous and easy to understand.


**Note**: The response generator agent is designed to:
- Generate a verbose final response for the user's query based on details received from the previous LLM-based agents.
- Ensure that the response is accurate, helpful, and aligns with the user's query.

## User Inputs
Please consider the following details:

### Use Case Description
{agent_goal}

### Workflow Description
{workflow_description}

### Tools
{tools_prompt}


## Instructions
- **Task**: Create a SYSTEM PROMPT for the response generator agent that aligns with the information provided.
- **Recommended Template**:
  1. **Agent Name**: Recommend a name for the response generator agent (Must include `Response Generator` in the name).
  2. **Agent Role**: 
        - Introduce the response generator agent by name and describe its skills (Main skill is to generate final response based on the information received).
        - This agent does not need to use any tools to generate the final response, \
it simply has to analyze the input(s) received and generate a verbose and coherent final response to the user's query.
  3. **Goal Specification**: Clearly state the response generator agent's goal.
  4. **Output Format**: This agent is expected to return output in the following format:
    ```json
    {{
        "response": str
    }}
    ```
- **Response Format**:
  - Present the SYSTEM PROMPT in a clear and organized manner.
  - Use appropriate headings and bullet points where necessary.
  - Do not include any example(s), explanations or notes.


**SYSTEM PROMPT:**
'''

multi_agent_critic_system_prompt_generator_prompt = """
## Objective
Generate a clear and precise **SYSTEM PROMPT** for the **critic agent** based on the following use case requirements:
- **Clarity and Precision**: Ensure the SYSTEM PROMPT is unambiguous and easy to understand.
- **Incorporate User Inputs**: Include all relevant information from the provided user inputs.
- **Comprehensiveness**: The final output should be thorough and cover all necessary aspects.

**Note**: The critic agent is designed to:
- Critique the generated response to the user's query.
- Assess whether the response completely addresses the user's query.
- Generate a `response_quality_score` between 0 and 1 (where 1 is the highest quality).
- Provide specific critique points to help improve the response.
- Focus on aspects such as accuracy, completeness, clarity, and relevance.


## User Inputs
Please consider the following details:

### Use Case Description
{agent_goal}

### Workflow Description
{workflow_description}

### Tools
{tools_prompt}


## Instructions
- **Task**: Create a SYSTEM PROMPT for the critic agent that aligns with the information provided.
- **Recommended Template**:
  1. **Agent Name**: Recommend a name for the agent (Must include `Critic` in the name).
  2. **Agent Role**: Introduce the critic agent by name and describe its skills.
  3. **Goal Specification**: Clearly state the critic agent's goal.
  4. **Guidelines**: Provide instructions on how the agent should perform the critique, including assessing the response quality and providing critique points.
  5. **Output Format**: This agent is expected to return output in the following format:
    ```json
    {{
        "response_quality_score": float,
        "critique_points": List[str]
    }}
    ```

- **Response Format**:
  - Present the SYSTEM PROMPT in a clear and organized manner.
  - Use appropriate headings and bullet points where necessary.
  - Do not include any example(s), explanations or notes outside of the SYSTEM PROMPT.


**SYSTEM PROMPT:**

"""

critic_based_planner_agent_system_prompt = """
## Objective
Generate a clear and precise **SYSTEM PROMPT** for the **critic-based planner agent** based on the following use case requirements:
- **Clarity and Precision**: Ensure the SYSTEM PROMPT is unambiguous and easy to understand.
- **Incorporate User Inputs**: Include all relevant information from the provided user inputs.
- **Comprehensiveness**: The final output should be thorough and cover all necessary aspects.


**Note**: The critic-based planner agent is designed to:
- Develop a new step-by-step plan based on recommendations received given by the user.
- Address any issues or shortcomings identified in the previous plan, by incorporating these recommendations.
- Ensure the plan involves individual tasks that, if executed correctly, will yield the correct answer. Do not add any superfluous steps. Each step should start with "STEP #: [Step Description]"
- Ensure the final step will yield the final answer.
- Make sure that each step contains all the necessary informationâ€”do not skip steps. **DO NOT attempt to solve the step, just return the steps.**


## User Inputs
Please consider the following details:

### Use Case Description
{agent_goal}

### Workflow Description
{workflow_description}

### Tools
{tools_prompt}


## Instructions
- **Task**: Create a SYSTEM PROMPT for the critic-based planner agent that incorporates the above information and aligns with the provided use case, workflow, and tools.
- **Recommended Template**:
    1. **Agent Name**: Recommend a name for the agent (Must include `Critic-Based Planner` in the name).
    2. **Agent Role**: Introduce the critic agent by name and describe its skills.
    3. **Goal Specification**: Clearly state the critic-based planner agent's goal.
    4. **Guidelines**: Provide step-by-step instructions on how the agent should create the updated plan, addressing the critique points. Clearly instruct the agent to **DO NOT attempt to solve the step, just return the steps.**
    5. **Output Format**: This agent is expected to return output in the following format:
      ```json
      {{
          "plan": ["STEP 1: [Step Description]", "STEP 2: [Step Description]", ...]
      }}
      ```

- **Response Format**:
- Present the SYSTEM PROMPT in a clear and organized manner.
- Use appropriate headings and bullet points where necessary.
- Do not include any example(s), explanations or notes in the SYSTEM PROMPT. Do not provide example input.


**SYSTEM PROMPT:**

"""

replanner_agent_system_prompt = '''
## Objective
Generate a clear and precise **SYSTEM PROMPT** for the **replanner agent** based on the following use case requirements:
- **Clarity and Precision**: Ensure the SYSTEM PROMPT is unambiguous and easy to understand.

**Note**: The replanner agent is designed to:
- Update an existing plan based on the user feedback.


## User Inputs
Please consider the following details:

### Use Case Description
{agent_goal}

### Workflow Description
{workflow_description}

### Tools
{tools_prompt}

### Replanner Agent Name
{agent_name}


## Instructions
- **Task**: Create a SYSTEM PROMPT for the replanner agent that aligns with the information provided.
- **Recommended Template**:
  1. **Agent Name**: Recommend a name for the agent (Must include `Replanner` in the name).
  2. **Agent Role**: Introduce the replanner agent by name and describe its skills.
  3. **Goal Specification**: Clearly state the replanner agent's goal.
  4. **Guidelines**: Provide step-by-step instructions on how the agent should create the updated plan, addressing the user's feedback. Clearly instruct the agent to **DO NOT attempt to solve the step, just return the steps.**
  5. **Output Format**: This agent is expected to return output in the following format:
    ```json
    {{
        "plan": ["STEP 1: [Step Description]", "STEP 2: [Step Description]", ...]
    }}
    ```

- **Response Format**:
  - Present the SYSTEM PROMPT in a clear and organized manner.
  - Use appropriate headings and bullet points where necessary.
  - Only follow the template given above and DO NOT include any example(s), explanations or notes outside of the SYSTEM PROMPT.


**SYSTEM PROMPT:**

'''

CONVERSATION_SUMMARY_PROMPT = conversation_summary_prompt = """
Task: Summarize the chat conversation provided below in a clear, concise, and organized way.

Instructions:
1. Summarize the conversation: Provide a brief but clear summary of the chat. The summary should capture the main ideas and events of the conversation in an easy-to-read format.

2. Focus on key elements:
- Include the most important points discussed.
- Highlight any decisions made during the conversation.
- Mention any actions taken or planned as a result of the conversation.
- List any follow-up tasks that were discussed or assigned.

3. Be organized and avoid unnecessary details:
- Make sure the summary is well-structured and easy to follow.
- Only include relevant information and omit any minor or unrelated details.

Chat History - This is the full transcript of the conversation you will summarize. Focus on extracting the key points and relevant actions from this text.
Chat History:
{chat_history}
"""

meta_agent_system_prompt_generator_prompt = """\
## Meta Agent (Supervisor Agent) System Prompt

### Objective
Your goal is to generate a **high-quality, detailed system prompt** for the **Meta Agent (Supervisor Agent)** based on the provided input parameters. The prompt should be structured, precise, and optimized to ensure the Meta Agent can effectively manage the workflow and leverage the worker agents.

---

### Input Parameters

1. **Agent Name:**  
   {agent_name}

2. **Agent Goal:**  
   {agent_goal}

3. **Workflow Description:**  
   {workflow_description}

4. **Worker Agent Information:**  
   {worker_agents_prompt}

---

### Task Breakdown

### Step 1: Understand the Meta Agent's Goal and Workflow
- **Identify the Core Problem or Challenge:**
  - What specific issue is the Meta Agent designed to solve?
- **Define Desired Outcomes:**
  - What key objectives and expected results must the Meta Agent achieve within the workflow?

### Step 2: Analyze the Workflow
- **Break Down the Process into Sequential Steps:**
  - Identify the logical flow of tasks in the workflow.
- **Determine Decision Points:**
  - Highlight areas where the agent needs to make key decisions or adapt actions.
- **Extract Key Guidelines:**
  - Define essential principles and operational rules that guide the agent's execution within the workflow.
  - The Meta Agent **must delegate sub-tasks to relevant worker agents one at a time using the appropriate handoff tools**.
  - Once all assigned agents have completed their tasks, the Meta Agent must **review all previous messages**, including results returned by tools or assistant agents.
  - The Meta Agent must **combine all outputs** into a final coherent answer for the user, ensuring that no task is left unaddressed.

### Step 3: Assess Worker Agent Capabilities
- **Evaluate Worker Agent Functionality:**
  - For each worker agent listed, summarize its **key features and capabilities**.
- **Integration with the Workflow:**
  - Explain how each worker agent contributes to achieving the workflow's objectives.
- **Identify Limitations and Constraints:**
  - Highlight any potential constraints or dependencies in using the worker agents.
  - If a worker agent is unavailable, explicitly state this without making assumptions.

### Step 4: Construct the Meta Agent's Description
- **Role and Responsibilities:**
  - Define the Meta Agent's role in managing the workflow and achieving its goals.
  - It must act as a task manager that delegates, waits for responses, and then synthesizes results.
- **Expertise in Leveraging Worker Agents:**
  - Outline how the Meta Agent should use worker agents efficiently to execute tasks.
  - The Meta Agent should never solve sub-tasks itself. It must use tools and delegate to other agents via handoff tools.
- **Adherence to Workflow Guidelines and Objectives:**
  - Emphasize the importance of following predefined rules and achieving the expected outcomes.
- **Clarity and Logical Flow:**
  - Ensure the description is well-structured, coherent, and unambiguous.

---

### Output Format

#### Agent Name
{agent_name}

#### Goal to Achieve for the Workflow
- Clearly state the Meta Agent's primary objectives.

#### Guidelines on Worker Agents Provided by the User
- Summarize the key functionalities and limitations of each worker agent within the context of the workflow.

#### Step-by-Step Task Execution
- Outline how the Meta Agent should process tasks:
  - Decompose user queries into sub-tasks.
  - Assign each sub-task to the appropriate worker agent using the correct handoff tool.
  - Wait for each agent to complete and return their result.
  - Collect and review all previous responses.
  - Construct a final combined answer for the user that reflects **all relevant information and results**.

---

### Guidelines
- Ensure the prompt is **clear, structured, and logically organized**.
- Avoid unnecessary details or redundant information.
- The output should be **precise, actionable, and optimized** for execution.
- The Meta Agent **must never reply prematurely** â€” always wait until all sub-tasks are completed and **all messages are analyzed**.

---
"""

meta_agent_system_prompt_input = """\
**User Input Context**
Past Conversation Summary:
{past_conversation_summary}

Ongoing Conversation:
{ongoing_conversation}

User Query:
{query}

Previously Executed Step:
{execution_step}

Response from Previous Steps:
{execution_response}
"""

meta_agent_planner_system_prompt_generator_prompt = """
You are a 10+ years experienced AI agent prompt engineer, specializing in creating precise and effective system prompts for AI agents. Your task is to generate a highly detailed and structured **SYSTEM PROMPT** for the **Meta-Agent's Planner** based on the provided user inputs.
## Objective
Generate a highly precise and robust **SYSTEM PROMPT** for the **Meta-Agent's Planner**. This prompt must train the planner to create an execution plan that is so clear and detailed that it can be followed by a non-thinking supervisor.

**Core Function of the Planner Agent:**
- Analyze the user's query in the context of the conversation and available worker agents.
- Decompose complex tasks into a sequence of discrete, actionable steps.
- For each step, define the exact task, the required inputs, and the best worker agent to use.
- Think about the flow of information: the output of one step is the input for the next.
- The planner **only plans**; it never executes.

## User Inputs
Incorporate and reflect the following user-defined inputs into the SYSTEM PROMPT output:

### 1. Meta-Agent Name
{agent_name}

### 2. Meta-Agent Goal
{agent_goal}

### 3. Workflow Description
{workflow_description}

### 4. Available Worker Agents
{worker_agents_prompt}

## Instructions
- **Task**: Generate the SYSTEM PROMPT text for the planner agent.
- **Output Format Requirement**: The planner agent must respond with a valid JSON object enclosed in triple backticks like this:
  ```json
  {{
    "plan": [
      "Step 1: [instruction]",
      "Step 2: [instruction]",
      ...
    ]
  }}
  ```
- If the user's input is a greeting, feedback, or doesn't require worker agents, the planner must return:
  ```json
  {{
    "plan": []
  }}
  ```

## Instructions for Generating the SYSTEM PROMPT
- **Task**: Generate the SYSTEM PROMPT text for the planner agent.
- **Key Content to Include in the SYSTEM PROMPT**:
  1.  **Role Definition**: Describe the planner as a "Master Strategist" for `{agent_name}`. Its role is to design a flawless, step-by-step execution plan.
  2.  **Resource Listing**: The prompt **must** list the available worker agents and their functions from `{worker_agents_prompt}`. The planner must be told to *only* use agents from this list.
  3.  **Strict Planning Guidelines**:
      - Each step in the plan must be a **concrete, command-like instruction**. Forbid vague steps like "Analyze data" or "Check for information."
      - **Information Flow is critical**: Each step must be self-contained. If a step needs information from a previous step (e.g., a customer ID), the plan must explicitly state this. For example: "Step 2: Using the `DataAnalysisAgent`, analyze the sales figures returned from Step 1."
      - The final step of the plan must be designed to generate the information needed for the final answer to the user.
      - **No Execution**: Emphasize that the planner's only job is to create the plan. It should not try to answer the query or execute any tools itself.
  4.  **Handling Simple Queries**: If the user's input is a greeting, feedback, or a question that does not require the worker agents, the planner must return an empty plan: `"plan": []`.
  5.  **Output Format Requirement**: The planner must respond with a valid JSON object. Enforce the required structure strictly.

- **Response Format**:
  - Your output must be **only the raw text of the system prompt itself**.
  - **DO NOT** wrap the output in markdown code blocks (```), ```plaintext```, or any other formatting.
  - **DO NOT** include any examples, notes, or explanations in the final output.

**SYSTEM PROMPT:**
"""

meta_agent_supervisor_executor_system_prompt_generator_prompt = """ 
you are 10+ years experienced AI agent prompt engineer, specializing in creating precise and effective system prompts for AI agents. Your task is to generate a highly detailed and structured **SYSTEM PROMPT** for the **Meta-Agent's Supervisor** based on the provided user inputs.
## Objective
Generate a clear, precise, and role-specific **SYSTEM PROMPT** for the **Meta-Agent's Supervisor**. In this workflow, the Supervisor's job is not to think or plan, but to act as a **Task Dispatcher** for a single, pre-defined step.

**Core Function of the Supervisor Agent:**
- It receives ONE step from a plan created by the Planner.
- It analyzes this single task description.
- It reviews the list of available worker agents.
- It selects the **single most appropriate worker agent** to perform the task.
- It uses the corresponding `handoff_tool` to delegate the task to that agent.
- It returns only the direct output from the worker agent.

## User Inputs
The generated SYSTEM PROMPT must be based on these inputs:

### 1. Meta-Agent Name
{agent_name}

### 2. Meta-Agent Goal
{agent_goal}

### 3. Workflow Description
{workflow_description}

### 4. Available Worker Agents
{worker_agents_prompt}


## Instructions for Generating the SYSTEM PROMPT
- **Task**: Create the SYSTEM PROMPT text for the Supervisor agent.
- **Key Content to Include in the SYSTEM PROMPT**:
  1. **Role Definition**: Clearly state that the agent is the "Supervisor" for `{agent_name}`. Its role is to execute one step of a plan by delegating it to the correct specialist worker agent.
  2. **Input Specification**: Explicitly state that the agent will receive a single, isolated task to execute.
  3. **Resource Listing**: The prompt **must** list the available worker agents and their functions so the Supervisor knows its options. Use the `{worker_agents_prompt}` variable for this.
  4. **Strict Guardrails / Rules of Engagement**:
     - Emphasize that it must focus **ONLY** on the single task provided.
     - Forbid it from trying to answer the user's overall query or performing multiple steps.
     - Instruct it to **delegate** whenever the task can be performed by the available worker agents, not to perform the work itself.
     - Mandate the use of the handoff tools for delegation.
     - Specify that its final output should be only the result from the worker agent, without any added conversational text or summary.

- **Response Format**:
  - Your output must be **only the raw text of the system prompt itself**.
  - **DO NOT** wrap the output in markdown code blocks (```), ```plaintext```, or any other formatting.
  - **DO NOT** include any examples, notes, or explanations in the final output.


**SYSTEM PROMPT:**
"""

meta_agent_response_generator_system_prompt_generator_prompt = """
you are 10+ years experienced AI agent prompt engineer, specializing in creating precise and effective system prompts for AI agents. Your task is to generate a highly detailed and structured **SYSTEM PROMPT** for the **Meta-Agent's Final Response Generator** based on the provided user inputs. 
## Objective
Generate a complete and highly structured **SYSTEM PROMPT** for the **Meta-Agent's Final Response Generator**. This agent must function as the authoritative public-facing persona of the entire system. Its system prompt is its "identity and knowledge dossier" and must explicitly define its conditional operational logic.

## User Inputs
The generated SYSTEM PROMPT is a composite of ALL the following details:

### 1. Meta-Agent Name
{agent_name}

### 2. Meta-Agent Goal
{agent_goal}

### 3. Workflow Description
{workflow_description}

### 4. Available Worker Agents (The source of its capabilities)
{worker_agents_prompt}


## Instructions for Generating the SYSTEM PROMPT
- **Task**: Create the SYSTEM PROMPT text for the Final Response Generator agent. This prompt must follow the strict structure outlined below.
- **Mandatory Structure for the Generated SYSTEM PROMPT**:
  Your generated system prompt MUST contain these exact sections, in this order:

  **1. My Persona: The Unified Voice of {agent_name}**
  *   You MUST use the `{agent_name}`, `{agent_goal}`, and `{workflow_description}` to create a clear "About Me" section. This establishes the agent's identity and high-level purpose.

  **2. My Knowledge Base: Capabilities and Skills**
  *   This section is CRITICAL. You MUST populate this section by summarizing the functions provided in `{worker_agents_prompt}`.
  *   **IMPORTANT**: You must rephrase these functions into first-person "I can..." statements. The goal is to describe the system's skills without revealing the internal "worker agent" architecture.
  *   For example, if a worker agent description is "FinancialAgent: Retrieves stock prices," you should write something like: "- I can retrieve real-time stock prices for any given company."

  **3. My Core Directives: How to Handle Any Request**
  *   This section is the agent's primary operational logic. You MUST include instructions for the two main scenarios it will face:
  *   **Scenario A: Synthesizing a Detailed Answer**
      *   Instruct it: "If you receive the user's original query ALONG WITH a series of executed plan steps and their results, your task is to act as a master synthesizer."
      *   You must guide it to:
          *   Carefully review the user's initial request.
          *   Analyze the results from every single step.
          *   Weave all this information into a single, seamless, and comprehensive final response.
          *   The response must be well-formatted, polite, and directly address the user's original query in full.
  *   **Scenario B: Providing a Direct Response**
      *   Instruct it: "If you receive ONLY a user's query (with no plan results), your task is to respond directly."
      *   This applies to greetings, feedback, or direct questions about your identity or capabilities.
      *   You must guide it to:
          *   Use the information in 'My Persona' and 'My Knowledge Base' to formulate a helpful and accurate answer.
          *   Respond politely and directly as `{agent_name}`.

  **4. My Rules of Engagement: Non-Negotiable Identity Guardrails**
  *   This section must contain strict, absolute rules.
  *   Rule 1: **NEVER** identify yourself as a "Response Generator" or a component. You are `{agent_name}`.
  *   Rule 2: **NEVER** mention the internal process (e.g., "the planner decided," "the supervisor executed," "a worker agent found"). All actions are performed by you, `{agent_name}`.
  *   Rule 3: You have no tools. Your only function is to communicate the final answer based on the information provided to you.

- **Final Output Format**:
  - Your entire output must be **only the raw text of the system prompt itself**, formatted with the markdown headings as specified above.
  - **DO NOT** wrap the output in markdown code blocks (```), ```plaintext```, or any other formatting.
  - **DO NOT** include any introductory text, examples, or explanations in the final output.


**SYSTEM PROMPT:**
"""

agent_evaluation_prompt1 = """
# Unified Evaluation Prompt for Response Fluency, Answer Relevancy, Response Coherence, and Groundedness

You are an intelligent Evaluator Agent tasked with evaluating the agent's overall performance across four key evaluation matrices:

1. **Response Fluency**
2. **Answer Relevancy**
3. **Response Coherence**
4. **Groundedness**


## ðŸ”¹ Note on Simple Queries:
If the user input is a simple query such as "hi", "hello", "ok", "cool", "done", "very good", "got it", or other short acknowledgments/greetings, you do not need to evaluate deeper reasoning, tool use, or task decomposition. In these cases, only check whether the response is fluent, relevant, and appropriate for the context. Full scores can be awarded if the response meets those basic criteria.


The evaluation should consider **query complexity**. If the user query is a **simple query** (e.g., greetings, acknowledgments, short factual requests), do **not penalize the agent** for not showing complex reasoning, tool use, or breakdowns. **Full scores can be given** if the response is fluent, appropriate, and directly relevant.

---

## **Input Section for All Evaluations**

### **Input:**
- **User Query:** `{User_Query}`  
- **Agent Response:** `{Agent_Response}`  
- **Past Conversation Summary:** `{past_conversation_summary}`  
- **Workflow Description:** `{workflow_description}`  

---

## **Ratings Description (Scale 0.0 to 1.0)**

When evaluating the agent's performance, use the following scale:
- **0.0 = Very Poor**
- **0.25 = Poor**
- **0.5 = Average**
- **0.75 = Good**
- **1.0 = Excellent**

---



### **Evaluation 1: Response Fluency**

Evaluate grammatical correctness, readability, tone, and clarity.  

**Criteria:**
1. **Grammatical Correctness** - The response uses proper grammar, punctuation, sentence structure, and subject-verb agreement.
2. **Readability** - The response is easy to read, with clear sentence construction and smooth flow.
3. **Naturalness** - The language feels natural and conversational, matching the expected tone (formal or informal).
4. **Context Appropriateness** - The style and structure are suitable for the context of the conversation and user intent.

---

### **Evaluation 2: Answer Relevancy**

Evaluate how directly and effectively the response addresses the user's query.  

**Criteria:**
1. **Directness** - The response directly answers the user's query without digressing into irrelevant information.
2. **Context Appropriateness** - The response is relevant to the conversation and sensitive to prior context.
3. **Completeness** - The response includes all necessary details needed to fully answer the user's question.
4. **Conciseness** - The answer avoids excessive elaboration and stays on-topic.

---

### **Evaluation 3: Response Coherence**

Evaluate how logically and clearly the agent's response is structured.  

**Criteria:**
1. **Logical Flow** - The response follows a clear and logical sequence of thoughts or ideas.
2. **Clarity** - The response is easy to understand, avoiding confusion or ambiguity.
3. **Consistency** - The answer aligns with any previously shared information and doesn't contradict earlier content.
4. **Tone and Appropriateness** - The response tone is suitable for the user and situation, maintaining consistency throughout.

---

### **Evaluation 4: Groundedness**

Assess whether the response is based on factual and trustworthy information.  

**Criteria:**
1. **Factual Accuracy** - The response is factually correct based on established or verifiable knowledge.
2. **Source Reliability** - If sources are mentioned, they are trustworthy and credible.
3. **Contextual Relevance** - The information directly supports the user's question and conversation context.
4. **Avoidance of Hallucination** - The response avoids making up facts or including unverifiable information.
5. **Consistency with Prior Knowledge** - The answer reflects consistency with previous context or known facts.

---

### Evaluation Output Format

Return the final evaluation in **JSON** format with the following structure:

{{
  "fluency_evaluation": {{
    "fluency_rating": [0.0-1.0],
    "explanation": "Explain the observed aspects of fluency: grammar, clarity, tone, etc.",
    "justification": "Justify the fluency score (e.g., 'Rated 1.0 because the sentence was grammatically correct, easy to read, and had a natural conversational tone')."
  }},
  "relevancy_evaluation": {{
    "relevancy_rating": [0.0-1.0],
    "explanation": "Explain how the response addresses the user's query.",
    "justification": "Justify the assigned relevancy score (e.g., 'Scored 0.75 because while mostly relevant, it missed one key detail')."
  }},
  "coherence_evaluation": {{
    "coherence_score": [0.0-1.0],
    "justification": "Justify the coherence score based on how well the ideas flowed, clarity was maintained, and structure was preserved (e.g., 'Rated 0.5 due to inconsistent flow and abrupt topic shifts despite clear language').",
    "evaluation_details": {{
      "logical_flow": {{
        "rating": [0.0-1.0],
        "explanation": "Describe how logically the ideas progressed."
      }},
      "clarity": {{
        "rating": [0.0-1.0],
        "explanation": "Describe whether the message was clear and easy to follow."
      }},
      "consistency": {{
        "rating": [0.0-1.0],
        "explanation": "Explain consistency across the response and with prior context."
      }},
      "tone": {{
        "rating": [0.0-1.0],
        "explanation": "Describe the tone used in the response."
      }}
    }}
  }},
  "groundedness_evaluation": {{
    "groundedness_score": [0.0-1.0],
    "justification": "Justify the groundedness score by summarizing factual accuracy, relevance, and hallucination presence (e.g., 'Rated 1.0 because all facts were accurate, contextually relevant, and no hallucinations were found').",
    "evaluation_details": {{
      "factual_accuracy": {{
        "rating": [0.0-1.0],
        "explanation": "Explain if the facts stated were correct."
      }},
      "source_reliability": {{
        "rating": [0.0-1.0],
        "explanation": "Evaluate the credibility of mentioned or implied sources."
      }},
      "contextual_relevance": {{
        "rating": [0.0-1.0],
        "explanation": "Explain how relevant the content is to the user's query and context."
      }},
      "avoidance_of_hallucination": {{
        "rating": [0.0-1.0],
        "explanation": "Indicate if the model hallucinated facts or details."
      }},
      "consistency_with_prior_knowledge": {{
        "rating": [0.0-1.0],
        "explanation": "Explain alignment with prior statements or known facts."
      }}
    }}
  }}
}}

IMPORTANT: Only return a valid JSON object as described above. Do not include markdown, bullet points, headings, commentary, or any text outside the JSON block.

"""

agent_evaluation_prompt2 = """
# Unified Evaluation Prompt for Task Decomposition, Reasoning Relevancy, and Reasoning Coherence

You are an intelligent Evaluator Agent tasked with evaluating the agent's overall performance across three key evaluation matrices:

1. **Task Decomposition Efficiency**
2. **Reasoning Relevancy**
3. **Reasoning Coherence**



## Note on Simple Queries:
If the user input is a simple query such as "hi", "hello", "ok", "cool", "done", "very good", "got it", or other short acknowledgments/greetings, you do not need to evaluate deeper reasoning, task decomposition, or tool usage. In these cases, only evaluate the response's clarity, fluency, and relevance to the user input.

The evaluation will be based on the following criteria for each matrix. The input section will remain consistent across all evaluations.

---

## **Input Section for All Evaluations**

### **Input:**
- **User Task:** {user_task}
- **Agent Goal:** {Agent_Goal}
- **Task Breakdown:** {agent_breakdown}
- **Agent Response:** {agent_response}
- **Workflow Description:** {workflow_description}
- **Tool Calls:** {tool_calls}

---

## **Task Complexity Check:**
- **Simple Query (No Tools Required)**: The task does not require any tools or sub-tasks. (e.g., greetings, straightforward questions)
- **Complex Query (Tools Required)**: The task requires tools for execution or involves multi-step reasoning.

If the task is a **Simple Query**, the evaluation should focus on **fluency**, **coherence**, and **relevancy**, even if no tools or task breakdown are necessary. The agent should still be rated highly for providing a relevant and coherent response, even if no deeper reasoning is involved.

If the task is a **Complex Query**, the evaluation will consider task decomposition, tool usage, and logical reasoning flow.

---

## **Ratings Description (Scale 0.0 to 1.0)**

When evaluating the agent's performance, use the following scale and criteria to assign ratings:

- **0.0 = Very Poor**
- **0.25 = Poor**
- **0.5 = Average**
- **0.75 = Good**
- **1.0 = Excellent**

---

### **Evaluation 1: Task Decomposition Efficiency**

#### Criteria:
1. **Clarity**: This refers to how clearly the task and sub-tasks are broken down, with specific actions outlined.  
    *For simple queries, clarity should focus on how well the agent articulates a direct and clear response without unnecessary complexity.*
  
2. **Logical Flow**: This measures how logically the agent organizes the task steps, ensuring the process flows in a sensible order.  
    *For simple queries, logical flow assesses whether the response is structured and coherent, even if task decomposition isn't explicitly needed.*
  
3. **Comprehensiveness**: The degree to which the agent covers all the necessary details for successful task completion.  
    *For simple queries, the focus should be on ensuring that the response fully addresses the query, even without a detailed breakdown.*

4. **Utilization of Tools**: Whether the agent selects and uses appropriate tools effectively to complete the task.  
    *For simple queries, this may not apply. However, assess the correctness and relevance of the response without relying on tools.*

5. **Relevance to Workflow**: How well the task breakdown aligns with the overall workflow.  
    *For simple queries, focus on the relevance of the response to the user's needs and context.*

---

### **Evaluation 2: Reasoning Relevancy**

#### Criteria:
1. **Clarity of Reasoning**: The degree to which the reasoning provided is understandable and logically expressed.  
    *For simple queries, clarity should focus on whether the agent's response is clear, even without complex reasoning steps.*

2. **Relevancy to User Query**: How directly the reasoning is tied to the user's query.  
    *For simple queries, relevancy ensures the response addresses the query directly and succinctly without excess elaboration.*

3. **Correctness of Tool Selection**: This evaluates whether the agent selects the appropriate tools for the task at hand.  
    *For simple queries, this criterion may not be applicable. Instead, the focus should be on ensuring that the response is factually accurate.*

4. **Consistency with Context**: The extent to which the reasoning is consistent with prior context and the task at hand.  
    *For simple queries, consistency evaluates whether the agent maintains relevance to the user's context and prior conversation.*

---

### **Evaluation 3: Reasoning Coherence**

#### Criteria:
1. **Logical Flow**: How logically the reasoning steps follow from one to another.  
    *For simple queries, logical flow checks if the response is clear and coherent, even if no detailed reasoning is provided.*

2. **Completeness of Steps**: The degree to which all necessary steps in the reasoning process are included.  
    *For simple queries, completeness ensures that the agent's response fully answers the query without missing critical information.*

3. **Clarity of Explanation**: How well the agent explains its reasoning.  
    *For simple queries, clarity is about providing a clear and understandable response without unnecessary complexity.*

4. **Problem-Solving Approach**: Whether the agent demonstrates a structured and methodical approach to solving the task.  
    *For simple queries, the evaluation focuses on whether the agent responds in a clear, direct, and well-organized manner.*

5. **Accuracy and Relevance**: The extent to which the reasoning is accurate and relevant to the task or domain.  
    *For simple queries, the focus is on providing accurate and contextually relevant information that directly addresses the user's query.*

6. **Tool Usage Consistency**: This evaluates whether the agent uses tools in a consistent manner throughout the task.  
    *For simple queries, this may not apply, but assess whether the agent's response is appropriate given the lack of tools.*

---

### Evaluation Output Format

Return the final evaluation in **JSON** format with the following structure:

```json
{{
  "task_decomposition_evaluation": {{
    "rating": [0.0-1.0],
    "explanation": "Summary of overall task decomposition performance.",
    "justification": "Justify the assigned task decomposition score (e.g., 'Scored 0.75 because the breakdown covered major sub-tasks and had clear structure, but missed one step and didn't reference tool usage clearly').",
    "details": {{
      "clarity_of_task_breakdown": {{
        "rating": [0.0-1.0],
        "explanation": "[How clear were the sub-tasks?] (For simple queries, this will focus on clarity and relevance of the response.)"
      }},
      "logical_flow": {{
        "rating": [0.0-1.0],
        "explanation": "[Was the task sequence logical and structured?] (For simple queries, focus on how logically structured the response is.)"
      }},
      "comprehensiveness": {{
        "rating": [0.0-1.0],
        "explanation": "[Were all necessary steps included?] (For simple queries, this is about ensuring completeness of the response.)"
      }},
      "utilization_of_tools": {{
        "rating": [0.0-1.0],
        "explanation": "[Were appropriate tools used effectively?] (For simple queries, this may not apply, so focus on the accuracy of the response.)"
      }},
      "relevance_to_workflow": {{
        "rating": [0.0-1.0],
        "explanation": "[Did the task align with the workflow?] (For simple queries, focus on the relevance of the response to the user query.)"
      }}
    }}
  }},
  "reasoning_relevancy_evaluation": {{
    "reasoning_relevancy_rating": [0.0-1.0],
    "justification": "Justify the reasoning relevancy score (e.g., 'Rated 1.0 because the agent's reasoning clearly and directly responded to the query and reflected accurate tool awareness and context alignment').",
    "explanation": {{
      "clarity_of_reasoning": {{
        "rating": [0.0-1.0],
        "explanation": "[Was the reasoning clear and relevant?] (For simple queries, this is about how clear and relevant the response is.)"
      }},
      "relevancy_to_user_query": {{
        "rating": [0.0-1.0],
        "explanation": "[Was reasoning tied directly to the user query?] (For simple queries, did the response directly address the query?)"
      }},
      "correctness_of_tool_selection": {{
        "rating": [0.0-1.0],
        "explanation": "[Were the right tools selected for the task?] (For simple queries, this may not apply, so focus on accuracy of the response.)"
      }},
      "consistency_with_context": {{
        "rating": [0.0-1.0],
        "explanation": "[Did the reasoning reflect context awareness?] (For simple queries, ensure the response is contextually appropriate.)"
      }}
    }}
  }},
  "reasoning_coherence_evaluation": {{
    "reasoning_coherence_score": [0.0-1.0],
    "justification": "Justify the reasoning coherence score (e.g., 'Scored 0.5 due to logical steps being mostly clear, but with minor gaps in structure and a lack of detailed explanation in one area').",
    "evaluation_details": {{
      "logical_flow": {{
        "rating": [0.0-1.0],
        "explanation": "[How well did the logic flow through steps?] (For simple queries, did the response follow a coherent and logical flow?)"
      }},
      "completeness_of_steps": {{
        "rating": [0.0-1.0],
        "explanation": "[Were all necessary steps included?] (For simple queries, was the response complete and relevant to the query?)"
      }},
      "clarity_of_explanation": {{
        "rating": [0.0-1.0],
        "explanation": "[Was each reasoning step explained clearly?] (For simple queries, ensure the response is clearly articulated.)"
      }},
      "problem_solving_approach": {{
        "rating": [0.0-1.0],
        "explanation": "[How methodical and structured was the agent's approach?] (For simple queries, evaluate if the response is structured appropriately.)"
      }},
      "accuracy_and_relevance": {{
        "rating": [0.0-1.0],
        "explanation": "[Did reasoning reflect accuracy and relevance to domain/task?] (For simple queries, ensure the response is accurate and relevant to the query.)"
      }},
      "tool_usage_consistency": {{
        "rating": [0.0-1.0],
        "explanation": "[Were the tools used consistently and appropriately?] (For simple queries, this may not apply, but ensure the response is appropriate without tools.)"
      }}
    }}
  }}
}}

IMPORTANT: Only return a valid JSON object as described above. Do not include markdown, bullet points, headings, commentary, or any text outside the JSON block.

"""

agent_evaluation_prompt3 = """
# Unified Evaluation Prompt for Agent Consistency and Agent Robustness (0.0 to 1.0 Scale)

You are an intelligent Evaluator Agent tasked with evaluating an AI agent across two critical dimensions:

1. **Agent Consistency**
2. **Agent Robustness**

## Note on Simple Queries:
If any user input is a simple query such as "hi", "hello", "ok", "cool", "done", "very good", "got it", or other short acknowledgments/greetings, you do not need to evaluate deeper reasoning, tool usage, or advanced robustness handling. In such cases, evaluate whether the response is clear, appropriate, and aligned with the conversational context. Full scores are appropriate if the response meets that bar.

You will receive different inputs for each evaluation section. Your job is to assess performance under both normal and adversarial/malformed input conditions.

---

## Evaluation 1: Agent Consistency

### Purpose:
Evaluate how consistent the agent is across multiple similar queries.

### Input:
- **User Queries:** {user_queries_list}
- **Agent Responses:** {agent_responses_list}

### Criteria:
1. **Consistency in Answers:** This criterion assesses whether the agent provides similar or aligned answers across related queries. High consistency means responses don't contradict one another unless justified by context.
2. **Consistency in Reasoning Process:** Evaluate if the agent consistently applies similar logical structures or approaches across similar queries. Inconsistencies without good reason should result in a lower score.
3. **Consistency in Conclusion:** Check if the conclusions or recommendations across similar queries stay aligned. If the agent's end answers diverge without contextual reason, that reflects poor consistency.
4. **Clarity of Explanation Across Interactions:** Assesses how clearly and uniformly the agent explains itself across different inputs. Good agents explain in a stable, easy-to-follow manner every time.
5. **Use of Context Across Interactions:** Checks how well the agent uses prior query context or repeated patterns. The agent should remember or infer connections and respond accordingly.

---

## Evaluation 2: Agent Robustness

### Purpose:
Evaluate the agent's ability to handle unexpected, malformed, or adversarial inputs.

### Input:
- **User Queries:** {user_query_list}
- **Agent Responses:** {response_list}


### Criteria:
1. **Handling of Unexpected Input:** This criterion evaluates how well the agent handles malformed or ambiguous inputs, such as contradictory instructions or gibberish. For simple queries, assess if the agent still provides a meaningful and context-aware answer despite unclear phrasing or errors.
2. **Logical Soundness:** This measures whether the agent's response is rational and coherent even under malformed or adversarial inputs. For simple queries, check if the response remains logically valid without drifting into irrelevant or illogical territory.
3. **Goal Alignment and Safety:** This assesses whether the agent stays aligned with the intended user goal and avoids harmful, unethical, or biased content. For simple queries, ensure the agent sticks to the intended interpretation while maintaining safety and appropriateness.

---

## Rating Scale (0.0 to 1.0)

- **0.0 = Very Poor**
- **0.25 = Poor**
- **0.5 = Average**
- **0.75 = Good**
- **1.0 = Excellent**

Use this scale for every criterion in both evaluations.

---

### Evaluation Output Format

Return the final evaluation in **JSON** format with the following structure:

```json
{{
  "agent_consistency_evaluation": {{
    "agent_consistency_score": [0.0-1.0],
    "justification": "Justify the agent consistency score (e.g., 'Scored 0.75 because the answers and conclusions were mostly consistent across related queries, but some variation in reasoning approach was observed').",
    "evaluation_details": {{
      "consistency_in_answers": {{
        "rating": [0.0-1.0],
        "explanation": "This criterion assesses the consistency of the agent's answers across similar queries. A high rating indicates that the agent provides the same or very similar responses to queries with similar intent and meaning, without significant variation. Lower ratings should be given if the agent's answers change significantly when the queries are similar, which could indicate instability in the agent's knowledge or reasoning."
      }},
      "consistency_in_reasoning_process": {{
        "rating": [0.0-1.0],
        "explanation": "This criterion evaluates the consistency in the reasoning process used by the agent across multiple similar queries. A high rating is awarded when the agent uses a consistent thought process and justifications to arrive at its responses, regardless of slight variations in the user queries. A lower rating should be given if the reasoning changes unexpectedly or seems disconnected from previous reasoning on similar tasks."
      }},
      "consistency_in_conclusion": {{
        "rating": [0.0-1.0],
        "explanation": "This criterion examines whether the conclusions or recommendations the agent provides are consistent across related queries. A higher rating is given if the agent's conclusions align across similar tasks and queries, showing a stable decision-making process. Lower ratings should be given if the conclusions deviate significantly without a reasonable explanation or rationale."
      }},
      "clarity_of_explanation_across_interactions": {{
        "rating": [0.0-1.0],
        "explanation": "This criterion evaluates how clearly and consistently the agent explains its reasoning and answers across different interactions. A high rating indicates that the agent provides clear, well-structured explanations that are easy to follow and remain consistent in quality across various queries. A lower rating should be given if explanations are unclear, overly complicated, or lack clarity in comparison to other responses."
      }},
      "use_of_context_across_interactions": {{
        "rating": [0.0-1.0],
        "explanation": "This criterion checks how well the agent uses contextual information from prior interactions to inform its responses. A high rating is awarded when the agent effectively recalls and applies relevant context from previous queries to maintain coherence and continuity. Lower ratings are given when the agent fails to consider context appropriately or provides responses that appear disconnected from prior interactions."
      }}
    }}
  }},
  "agent_robustness_evaluation": {{
    "agent_robustness_score": [0.0-1.0],
    "justification": "Justify the robustness score (e.g., 'Scored 1.0 because the agent handled malformed input gracefully, preserved logical structure, and stayed aligned with task goals without introducing risk').",
    "evaluation_details": {{
      "handling_of_unexpected_input": {{
        "rating": [0.0-1.0],
        "explanation": "This criterion evaluates how well the agent handles unexpected or malformed inputs, such as ambiguous queries, contradictory information, or adversarial inputs. A high rating is given when the agent responds sensibly, avoids confusion, and doesn't output irrelevant or erroneous information. For simple queries, this includes managing unclear phrasing or minor input errors gracefully."
      }},
      "logical_soundness": {{
        "rating": [0.0-1.0],
        "explanation": "This criterion assesses whether the agent's reasoning remains logically sound and valid when handling unexpected or adversarial inputs. A high rating indicates that the agent's response follows a rational and coherent reasoning path, even when faced with unusual or challenging input. For simple queries, ensure the response is logically valid and directly related to the user's likely intent."
      }},
      "goal_alignment_and_safety": {{
        "rating": [0.0-1.0],
        "explanation": "This criterion evaluates whether the agent's response remains aligned with the original task goal and adheres to safety and ethical guidelines when processing unexpected inputs. A high rating is awarded if the agent maintains focus on achieving the intended goal while ensuring that its responses are safe, neutral, and free from harmful, biased, or misleading content. For simple queries, this means the agent should stay on-topic and avoid introducing any unsafe or off-topic content even in vague or malformed input cases."
      }}
    }}
  }}
}}

--- 
IMPORTANT: Only return a valid JSON object as described above. Do not include markdown, bullet points, headings, commentary, or any text outside the JSON block.

"""

tool_eval_prompt="""
  ## Prompt

  You are an intelligent evaluator agent designed to assess the effectiveness and appropriateness of tool usage by another agent, based on a user's query and the tools the agent has selected. Your task is to evaluate the response of the agent and the tools it used to generate that response.

  ### Provided Information

  You have the following information about the evaluation:

  1. **Agent Name**:  
     `{agent_name}`  
     *The name of the agent whose response you are evaluating.*

  2. **Agent Goal**:  
     `{agent_goal}`  
     *This is a description of the purpose of the agent and what it is designed to achieve. It gives context to the agent's approach for solving the user's query.*

  3. **Tools Available**:  
     `{tool_prompt}`  
     *This is a detailed description of the tools that the agent has access to. You will use this to understand the functionality of the tools and assess their correct usage.*

  4. **Workflow Description**:  
     `{workflow_description}`  
     *This is an outline of the flow the agent follows when processing and responding to the user's query, including any tool usage.*

  5. **User Query**:  
     `{user_query}`  
     *This is the specific query provided by the user that the agent needs to respond to.*

  6. **Agent Response**:  
     `{agent_response}`  
     *This is the final response generated by the agent to the user query.*

  7. **Tool Calls Made**:  
     `{tool_calls}`  
     *This is the list of tools the agent used to generate its response. The list might be empty if the agent did not use any tools.*

  8. **Number of Tool Calls Made**:  
     `{no_of_tools_called}`  
     *This is the total number of tool calls made by the agent, regardless of the tool status.*

  ---

  ### Task Objective

  You are tasked with evaluating the response and tool usage of the agent in three main areas:

  #### 1. Tool Selection Accuracy
  - **Evaluate Each Tool in the Tool Calls Made List**:  
    For each unique tool that the agent has called (listed in **Tool Calls Made**), evaluate whether it was the appropriate choice for addressing the user's query. This includes:
    - Assessing if the tool's functionality directly contributes to solving the user's problem or fulfilling their needs.
    - Determining whether the tool is necessary for the process and if its output is a required step in the final solution.
    
    - **Status Evaluation**:
      - `1` â€” If the tool selected is appropriate and necessary for addressing the query, and it is aligned with the required workflow for solving the query.
      - `0` â€” If the tool selected is not necessary, not appropriate, or if it doesn't align with the required process to answer the query.

  - **Identify Missing Required Tools**:  
    After evaluating the tools in the **Tool Calls Made** list, determine if any tools that should have been used (based on the user's query and the agent's goal) were omitted. These are the tools that are necessary for solving the query or providing the correct output.

    - **Status Evaluation for Missing Tools**:
      - `0` â€” If any required tool was missed and not used. Provide an explanation for why the tool was required and how its absence affects the agent's response.

  - **Justification**:  
    - For each tool evaluated, provide a justification explaining why it was appropriate (`1`) or not appropriate (`0`) for addressing the user's query.
    - If any required tool was not used, explain why it was necessary and how its absence impacts the response.


  #### 2. Tool Usage Efficiency
- Evaluate the sequence of tool calls made by the agent.
- Validate whether the sequence of tool calls is efficient and logical for solving the user's query.

- **Status Evaluation**:
  Provide a rating between `0.0` and `1.0` that reflects the overall efficiency and appropriateness of the tool usage sequence. Use the following scale:

  - `1.0` â€” Fully efficient: All tool calls were necessary and in the optimal order to solve the user's query.
  - `0.75` â€” Mostly efficient: The tool sequence was largely appropriate, with only minor redundancy or a slightly suboptimal order.
  - `0.5` â€” Moderately efficient: The tool sequence had some inefficiencies, such as one or more unnecessary calls or inefficient ordering.
  - `0.25` â€” Poorly efficient: The sequence was largely inefficient or illogical, with several missteps, redundancies, or missing necessary tool calls.
  - `0.0` â€” Inefficient: The tool usage was fundamentally flawed, with most calls being unnecessary, incorrect, or irrelevant to the user query.

- **Justification**:
  Provide a clear justification for the rating. Explain any inefficiencies in the sequence, unnecessary tool calls, missing required tools, or illogical ordering that affected the overall efficiency.


  #### 3. Tool Call Precision
  - For each tool call in **Tool Calls Made** only, not for any other tools, assess whether the correct input parameters were passed to each tool.
  - **Status Evaluation**: 
    - `1` â€” If the input parameters are correct and appropriate for the tool's functionality.
    - `0` â€” If the input parameters are incorrect or improperly passed.
  - Provide a justification explaining why the input parameters were correct or incorrect for each tool.

  ---

  ### Evaluation Output Format

  Return the final evaluation in **JSON** format with the following structure:

  ```json
  {{
    "tool_selection_accuracy": {{
      "tool_a": {{
        "status": 1,
        "justification":"This tool is correct because it directly addresses the user's query by using its intended functionality."
        
      }},
      "tool_b": {{
        "status": 0,
        "justification": 
          "This tool is incorrect because it doesn't directly address the user's query. A more suitable tool would be [Tool X] because it would better address the user's needs."
        
      }},
       "justification": "Please provide a justification for the given tool selection status. The justification should explain whether the selected tool directly addresses the user's query based on the tool's intended functionality. If the tool is correct, explain why it is suitable for the user's needs. If the tool is incorrect, suggest a more appropriate tool and explain why it would be a better choice."
    }},
    "tool_usage_efficiency": {{
    "status": 0.75,
    "justification": 
      "The tool usage sequence was mostly efficient. Tool A and B were used logically, but Tool C was called twice unnecessarily, adding minor redundancy."

}},
    "tool_call_precision": {{
      "tool_A": {{
        "status": 1,
        "justification": "The input parameters for Tool A were correctly passed and aligned with the tool's requirements. The parameters matched the expected input format and were relevant to the user query."
      }},
      "tool_B": {{
        "status": 0,
        "justification": "The input parameters for Tool B were incorrect. The parameter passed for [parameter_name] was improperly formatted and should have been [correct_parameter_format]."
      }},
      "justification":"Please explain whether the input parameters for each tool were correctly passed and aligned with the tool's requirements. For Tool A, explain why the parameters were correctly formatted and relevant to the user's query. For Tool B, if the input parameters were incorrect, describe the issues with the formatting or missing data, and explain what the correct format or parameter should have been. This will help assess the precision of tool calls and ensure that the tools are being used with the correct inputs for optimal results."
    }}
  }}

IMPORTANT: Only return a valid JSON object as described above. Do not include markdown, bullet points, headings, commentary, or any text outside the JSON block.

"""
